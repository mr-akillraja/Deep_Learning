{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML Metadata\n",
        "## Managing records in the production which can be later used to re-work , re-train , re-collect the dataset , and maintain the model according to the changes in the world. Storing MetaData in the MetaData Store in the machine learning pipeline . It will be created along with the creation of pipelines."
      ],
      "metadata": {
        "id": "hbw93AxB7EGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tfx"
      ],
      "metadata": {
        "id": "gzOqJhQT4ZAQ",
        "outputId": "83fc34cb-3db9-48fe-e6be-53d74c15a282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tfx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
            "Requirement already satisfied: ml-pipelines-sdk==1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
            "Requirement already satisfied: ml-metadata<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
            "Requirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.10/dist-packages (from tfx) (20.9)\n",
            "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.20.3)\n",
            "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.4.4)\n",
            "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.5.31)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.12.11)\n",
            "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.5.0)\n",
            "Requirement already satisfied: apache-beam[gcp]<3,>=2.47 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.50.0)\n",
            "Requirement already satisfied: attrs<22,>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (21.4.0)\n",
            "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
            "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.34.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<3,>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.34.4)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.59.0)\n",
            "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.4)\n",
            "Requirement already satisfied: kubernetes<13,>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (12.0.1)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.23.5)\n",
            "Requirement already satisfied: pyarrow<11,>=10 in /usr/local/lib/python3.10/dist-packages (from tfx) (10.0.1)\n",
            "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.1)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.13.1)\n",
            "Requirement already satisfied: tensorflow-hub<0.14,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.13.0)\n",
            "Requirement already satisfied: tensorflow-data-validation<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-model-analysis<0.46.0,>=0.45.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.45.0)\n",
            "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.13.1)\n",
            "Requirement already satisfied: tensorflow-transform<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
            "Requirement already satisfied: tfx-bsl<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.14.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.7)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.9.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.8.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.19)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.7.2)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
            "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.6.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (4.5.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.22.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.31.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.21.0)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.3.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.18.4)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.8.3)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.22.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.40.1)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.12.3)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.9.1)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.11.4)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.4.4)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.10.5)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.6.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.60.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.10.4)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.8.5.post1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.3)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (0.1.7)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (1.0.5)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2023.7.22)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (67.7.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.0.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging<21,>=20->tfx) (3.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tfx) (0.34.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.3.2)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.5.3)\n",
            "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (0.3.2)\n",
            "Requirement already satisfied: tensorflow-metadata<1.15,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.15.0,>=1.14.0->tfx) (1.14.0)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (9.4.0)\n",
            "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.11.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tfx) (0.41.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.12.6)\n",
            "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx) (6.5.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tfx) (0.6.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.8.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.0.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.5.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tfx) (2.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tfx) (3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tfx) (3.0.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-tuner<2,>=1.0.4->tfx) (13.6.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-tuner<2,>=1.0.4->tfx) (0.0.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-tuner<2,>=1.0.4->tfx) (0.1.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.8)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-tuner<2,>=1.0.4->tfx) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-tuner<2,>=1.0.4->tfx) (0.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.3.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.11.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.18.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (4.17.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (21.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.19.3)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (3.7.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.46.0,>=0.45.0->tfx) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1P_8O-pMsQKG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "import urllib\n",
        "import zipfile\n",
        "import tensorflow_data_validation as tfdv\n",
        "from ml_metadata.metadata_store import metadata_store\n",
        "from ml_metadata.proto import metadata_store_pb2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the zip file from GCP and unzip it\n",
        "url = 'https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip'\n",
        "zip, headers = urllib.request.urlretrieve(url)\n",
        "zipfile.ZipFile(zip).extractall()\n",
        "zipfile.ZipFile(zip).close()\n",
        "\n",
        "print(\"Here's what we downloaded:\")\n",
        "!ls -R data"
      ],
      "metadata": {
        "id": "eVNNpsD6pqxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4506d542-c2ad-4d29-c73f-4be66cfa6e06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's what we downloaded:\n",
            "data:\n",
            "eval  serving  train\n",
            "\n",
            "data/eval:\n",
            "data.csv\n",
            "\n",
            "data/serving:\n",
            "data.csv\n",
            "\n",
            "data/train:\n",
            "data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"TF version : {tf.__version__}\")\n",
        "print(f\"tensorflow validation version : {tfdv.version.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjLxlgZf5rKS",
        "outputId": "d5f30c1a-9f70-47dd-b582-074d594d1008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version : 2.13.1\n",
            "tensorflow validation version : 1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting from scratch will be defining each component of the data model.The outline will be :\n",
        "1. Defining the ML Metadata's store database.\n",
        "2. Setting up the necessary artifacts types.\n",
        "3. Setting up the execution types.\n",
        "4. Generating an input artifact unit.\n",
        "5. Generating an execution unit.\n",
        "6. Registering an input event.\n",
        "7. Running the TFDV component\n",
        "8. Generating an output aritfact unit\n",
        "9. Updating the execution unit.\n",
        "10. updating the execution unit\n",
        "11. setting up and generating a context unit.\n",
        "12. Generating attributions and associations."
      ],
      "metadata": {
        "id": "MeRp6gzJVfKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define ML Metastore's storage Database\n",
        "- instantiate the storage in the backend.\n",
        "- there are serveral types supported such as fake database,SQLite,MySQL and even cloud-based storage.\n",
        "- Here we are using a fake database."
      ],
      "metadata": {
        "id": "g58iWtcQWTTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a connection config\n",
        "connection_config = metadata_store_pb2.ConnectionConfig()\n",
        "\n",
        "# Set an empty fake database proto\n",
        "connection_config.fake_database.SetInParent()\n",
        "\n",
        "# setup the metadata store\n",
        "store = metadata_store.MetadataStore(connection_config)"
      ],
      "metadata": {
        "id": "dN-VVobk3V-G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register ArtifactTypes\n",
        "- will have to create the artifact type needed and register them to the store.\n",
        "- Generating a schema usin TFDV will only create two artifact types : - input dataset - output schema."
      ],
      "metadata": {
        "id": "D7Ni8pHbY3TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Artifact for the input dataset\n",
        "\n",
        "# Creates a new instance for the artifactype\n",
        "data_artifact_type = metadata_store_pb2.ArtifactType()\n",
        "\n",
        "# Sets the name for the data_artifact_type\n",
        "data_artifact_type.name = \"DataSet\"\n",
        "\n",
        "# The properties are the additional attributes associated with the artifactype.\n",
        "# This below line refers that the dataset can have the property named \"name\" and the value of the property is expected to be \"string\"\n",
        "data_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
        "# This below line refers that the dataset can have the property named \"split\" and the value of the property is expected to be \"String\"\n",
        "data_artifact_type.properties['split'] = metadata_store_pb2.STRING\n",
        "# This below line refer that the dataset can have the property named \"version\" and the value of the property is expected to be \"INT\"\n",
        "data_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
        "\n",
        "# Storing the artifact type,name and its property to the metastore.\n",
        "# We are registering the artifact to the metadata store\n",
        "data_artifact_type_id = store.put_artifact_type(data_artifact_type)\n",
        "\n",
        "# Create ArtifactType for schema : used to define the artifact within the metadata store.\n",
        "schema_artifact_type = metadata_store_pb2.ArtifactType()\n",
        "schema_artifact_type.name = 'Schema'\n",
        "schema_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
        "schema_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
        "\n",
        "# Register the artifact type to the metadata store\n",
        "schema_artifact_type_id = store.put_artifact_type(schema_artifact_type)\n",
        "\n",
        "print(f\"Data Artifact type : {data_artifact_type}\")\n",
        "print(f\"schema_Artifact type : {schema_artifact_type}\")\n",
        "print(f\"Data artifact type id : {data_artifact_type_id}\")\n",
        "print(f\"Schema artifact type id : {schema_artifact_type_id}\")"
      ],
      "metadata": {
        "id": "kcK2f8yo4DfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32126958-f566-4254-f60b-fb53dfce5fa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Artifact type : name: \"DataSet\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"split\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "\n",
            "schema_Artifact type : name: \"Schema\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "\n",
            "Data artifact type id : 10\n",
            "Schema artifact type id : 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Execution Type\n",
        "- create the execution types needed for the machine learning pipelines.\n",
        "- declare one component with data validation property so that you can record if the process is running or already completed."
      ],
      "metadata": {
        "id": "g0NcABB6yw9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ExecutionType for data validation component\n",
        "dv_execution_type = metadata_store_pb2.ExecutionType()\n",
        "dv_execution_type.name = 'Data Validation'\n",
        "dv_execution_type.properties['state'] = metadata_store_pb2.STRING\n",
        "\n",
        "# Register execution type to the metadata store\n",
        "dv_execution_type_id = store.put_execution_type(dv_execution_type)\n",
        "\n",
        "print(f\"data validation : {dv_execution_type}\")\n",
        "print(f\"data validation id : {dv_execution_type_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dtWzu9pZm8E",
        "outputId": "66877a35-b8d9-4418-b4cb-5f7a81ccfdc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data validation : name: \"Data Validation\"\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value: STRING\n",
            "}\n",
            "\n",
            "data validation id : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate input artifact unit\n",
        "- with the artifact created,now create instance of those types.\n",
        "- this artifacts is recorded in the metadata store.\n",
        "- again it generated the id that can be used for reference."
      ],
      "metadata": {
        "id": "ItOQJEO50Q6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the input artifact of type dataset\n",
        "data_artifact = metadata_store_pb2.Artifact()\n",
        "data_artifact.uri = './data/train/data.csv'\n",
        "data_artifact.type_id = data_artifact_type_id\n",
        "data_artifact.properties['name'].string_value = 'Chicago Taxi Dataset'\n",
        "data_artifact.properties['split'].string_value = 'Train'\n",
        "data_artifact.properties['version'].int_value = 1\n",
        "\n",
        "# Submit the input artifact to the metadata store\n",
        "data_artifact_id = store.put_artifacts([data_artifact])[0]\n",
        "\n",
        "print(f\"data artifact : {data_artifact}\")\n",
        "print(f\"data artifact id : {data_artifact_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5sqzvh1z70h",
        "outputId": "671ce763-1766-424f-a888-03cd855d3498"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data artifact : type_id: 10\n",
            "uri: \"./data/train/data.csv\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago Taxi Dataset\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"split\"\n",
            "  value {\n",
            "    string_value: \"Train\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "\n",
            "data artifact id : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate execution unit\n",
        "- will create the instance for the data validation execution type which have been registered earlier.\n",
        "- set the state to RUNNING to signify that you are about to run the TFDV function."
      ],
      "metadata": {
        "id": "WTVTSuAB243t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the execution of a data validation run\n",
        "dv_execution = metadata_store_pb2.Execution()\n",
        "dv_execution.type_id = dv_execution_type_id\n",
        "dv_execution.properties['state'].string_value = \"RUNNING\"\n",
        "\n",
        "# Submit the execution unit to the metadata store\n",
        "\n",
        "dv_execution_id = store.put_executions([dv_execution])[0]\n",
        "\n",
        "print(f\"data validation Execution : {dv_execution}\")\n",
        "print(f\"data validation Execution id : {dv_execution_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGyipyEv2w5d",
        "outputId": "3defbf12-7840-48e4-a1e0-50ffcbba0d6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data validation Execution : type_id: 12\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"RUNNING\"\n",
            "  }\n",
            "}\n",
            "\n",
            "data validation Execution id : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Register Input Event\n",
        "- event define the relationships between the artifact and the executions.\n",
        "- Generate the input event relationship for dataset artifact and data validation execution units."
      ],
      "metadata": {
        "id": "bF3V7RPS4XBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the input event\n",
        "input_event = metadata_store_pb2.Event()\n",
        "input_event.artifact_id = data_artifact_id\n",
        "input_event.execution_id = dv_execution_id\n",
        "input_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
        "\n",
        "# Submit the event to the metadata store\n",
        "store.put_events([input_event])\n",
        "\n",
        "print(f\"Input Event : {input_event}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROj_Y43U3fbn",
        "outputId": "1de26146-a030-4c0e-bc7d-d142e206f0a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Event : artifact_id: 1\n",
            "execution_id: 1\n",
            "type: DECLARED_INPUT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the TFDV Component\n",
        "- Run the tfdv component to generate the schema of dataset."
      ],
      "metadata": {
        "id": "ZOeSFbpl_FeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = './data/train/data.csv'\n",
        "train_stats = tfdv.generate_statistics_from_csv(train_data)\n",
        "schema = tfdv.infer_schema(statistics = train_stats)\n",
        "\n",
        "schema_file = './schema.pbtxt'\n",
        "tfdv.write_schema_text(schema,schema_file)\n",
        "\n",
        "print(f\"Dataset's Schema has been generated at : {schema_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "m3gJFpHf5I5m",
        "outputId": "74382bf3-54a1-43a1-d196-71a6a9486e7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset's Schema has been generated at : ./schema.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate output artifact unit\n",
        "- That the TFDV component has finished running and schema has been generated, you can create the artifact for the generated schema"
      ],
      "metadata": {
        "id": "PRYcdbIQQiKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the output artifact of type schema_artifact\n",
        "\n",
        "schema_artifact = metadata_store_pb2.Artifact()\n",
        "schema_artifact.uri = schema_file\n",
        "schema_artifact.type_id = schema_artifact_type_id\n",
        "schema_artifact.properties['version'].int_value = 1\n",
        "schema_artifact.properties['name'].string_value = \"Chicago taxi schema\"\n",
        "\n",
        "# Submit to the metadata store\n",
        "schema_artifact_id = store.put_artifacts([schema_artifact])[0]\n",
        "\n",
        "# Print the artifact schema id and display it\n",
        "print(f\"schema artifact : {schema_artifact}\")\n",
        "print(f\"schema artifact id : {schema_artifact_id}\")"
      ],
      "metadata": {
        "id": "0fisDBf_ASt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfe0b75-297b-46d4-8cd3-d045dde66413"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "schema artifact : type_id: 11\n",
            "uri: \"./schema.pbtxt\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago taxi schema\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "\n",
            "schema artifact id : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register output event\n",
        "- want to define an output event to record the output artifact of a particular execution event in the machine learning pipeline"
      ],
      "metadata": {
        "id": "ZqO-myI-TuX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the output event\n",
        "output_event = metadata_store_pb2.Event()\n",
        "output_event.artifact_id = schema_artifact_id\n",
        "output_event.execution_id = dv_execution_id\n",
        "output_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n",
        "\n",
        "# Submit to the metadata store\n",
        "store.put_events([output_event])\n",
        "\n",
        "print(f\"Output event : {output_event}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXvbpfleRVhZ",
        "outputId": "93506d20-f2b3-4cf5-d902-2dda2e933430"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output event : artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the execution unit\n",
        "- As the TFDV component has finished running successfully,you need to update the state of the executioni unit and record it again to the store."
      ],
      "metadata": {
        "id": "CzPcUxFKVK-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make the 'state' as 'COMPELETED'\n",
        "dv_execution_id = dv_execution_id\n",
        "dv_execution.properties['state'].string_value = 'COMPLETED'\n",
        "\n",
        "# Update the execution unit in the metadata store\n",
        "store.put_executions([dv_execution])\n",
        "\n",
        "print(f\"Data Validation execution : {dv_execution}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FG5WY7AU1JW",
        "outputId": "f830d842-5471-40c4-af5f-02eb066c6134"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Validation execution : type_id: 12\n",
            "properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"COMPLETED\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up context types and Generating a context unit\n",
        "- can group the artifacts and execution units into a context form .\n",
        "- first need to define the contextType.\n",
        "- Then we have to register."
      ],
      "metadata": {
        "id": "VNEnb7clYXXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a context type\n",
        "expt_context_type = metadata_store_pb2.ContextType()\n",
        "expt_context_type.name = \"Experiment\"\n",
        "expt_context_type.properties['note'] = metadata_store_pb2.STRING\n",
        "\n",
        "# Register context type to the metadata store\n",
        "expt_context_type_id = store.put_context_type(expt_context_type)"
      ],
      "metadata": {
        "id": "jOGdbhiVWsub"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the context\n",
        "expt_context = metadata_store_pb2.Context()\n",
        "expt_context.type_id = expt_context_type_id\n",
        "\n",
        "# Give the experiment a name\n",
        "expt_context.name = \"Demo\"\n",
        "expt_context.properties['note'].string_value = \"Walkthrough of metadata\"\n",
        "\n",
        "# Submit the context to the metadata store\n",
        "expt_context_id = store.put_contexts([expt_context])[0]\n",
        "\n",
        "# Print the context and experimented context\n",
        "print(f\"experiment context type : {expt_context_type}\")\n",
        "print(f\"experiment context type id : {expt_context_type_id} \")\n",
        "\n",
        "print(f'experiment context : {expt_context}')\n",
        "print(f\"experiment context id : {expt_context_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiBQGKLaZTOj",
        "outputId": "0b790397-cae3-4fec-96b0-742f8c66cff4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experiment context type : name: \"Experiment\"\n",
            "properties {\n",
            "  key: \"note\"\n",
            "  value: STRING\n",
            "}\n",
            "\n",
            "experiment context type id : 13 \n",
            "experiment context : type_id: 13\n",
            "name: \"Demo\"\n",
            "properties {\n",
            "  key: \"note\"\n",
            "  value {\n",
            "    string_value: \"Walkthrough of metadata\"\n",
            "  }\n",
            "}\n",
            "\n",
            "experiment context id : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate attribution and association relationships\n",
        "- To create the relationship between schema artifact unit and experiment context unit to form an attribution.\n",
        "- similarly , create the relationship between data validation execution unit and experiment unit to form an Association.\n",
        "- These are to be registered."
      ],
      "metadata": {
        "id": "S2rQfXqDbtgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.internals.blocks import extend_blocks\n",
        "# Generate the attribution\n",
        "expt_attribution = metadata_store_pb2.Attribution()\n",
        "expt_attribution.artifact_id = schema_artifact_id\n",
        "expt_attribution.context_id = expt_context_id\n",
        "\n",
        "# Generate the association\n",
        "expt_association = metadata_store_pb2.Association()\n",
        "expt_association.execution_id = dv_execution_id\n",
        "expt_association.context_id = expt_context_id\n",
        "\n",
        "\n",
        "# Submit attribution and association to the metadata store\n",
        "store.put_attributions_and_associations([expt_attribution],[expt_association])\n",
        "\n",
        "# Print the association and attribution\n",
        "print(f\"Experiment Attribution : {expt_attribution}\")\n",
        "print(f\"Experiment Association : {expt_association}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIihDd1PbWnn",
        "outputId": "5a1b1df4-7271-4f10-fd05-b61bb1ae206e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment Attribution : artifact_id: 2\n",
            "context_id: 1\n",
            "\n",
            "Experiment Association : execution_id: 1\n",
            "context_id: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrive the information from the metadata store\n",
        "- now we have recorded the needed information to the metadata store.\n",
        "- can track which artifacts and events are related to each other even without seeing the code used to generate it."
      ],
      "metadata": {
        "id": "e1LgAWOrlbxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the artifact types\n",
        "store.get_artifact_types()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niMhYCBedjCS",
        "outputId": "f6e9e221-0272-424e-e1b0-c4f92cdec95d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[id: 10\n",
              " name: \"DataSet\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"split\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value: INT\n",
              " },\n",
              " id: 11\n",
              " name: \"Schema\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value: STRING\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value: INT\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the 1st element in the list of schmea artifacts\n",
        "# Will investigate which dataset was used to generate it\n",
        "\n",
        "schema_to_inv = store.get_artifacts_by_type('Schema')[0]\n",
        "# print output\n",
        "print(schema_to_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf2tnAvgl_0u",
        "outputId": "829f1dd3-1ca6-4d9f-c8a7-558cd4faf46d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: 2\n",
            "type_id: 11\n",
            "uri: \"./schema.pbtxt\"\n",
            "properties {\n",
            "  key: \"name\"\n",
            "  value {\n",
            "    string_value: \"Chicago taxi schema\"\n",
            "  }\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "type: \"Schema\"\n",
            "create_time_since_epoch: 1696623512974\n",
            "last_update_time_since_epoch: 1696623512974\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get events related to the schema id\n",
        "schema_events = store.get_events_by_artifact_ids([schema_to_inv.id])\n",
        "\n",
        "print(schema_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSIrGTuRmaEd",
        "outputId": "dd29c5a7-432b-4be2-f103-500dbdba1401"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "milliseconds_since_epoch: 1696623512993\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(schema_events[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfQfo0N8rWsi",
        "outputId": "05cd71f3-9a91-4ef0-b113-9c9c79d2dbf6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ByteSize', 'Clear', 'ClearExtension', 'ClearField', 'CopyFrom', 'DECLARED_INPUT', 'DECLARED_OUTPUT', 'DESCRIPTOR', 'DiscardUnknownFields', 'Extensions', 'FindInitializationErrors', 'FromString', 'HasExtension', 'HasField', 'INPUT', 'INTERNAL_INPUT', 'INTERNAL_OUTPUT', 'IsInitialized', 'ListFields', 'MergeFrom', 'MergeFromString', 'OUTPUT', 'PENDING_OUTPUT', 'ParseFromString', 'Path', 'RegisterExtension', 'SerializePartialToString', 'SerializeToString', 'SetInParent', 'Type', 'UNKNOWN', 'UnknownFields', 'WhichOneof', '_CheckCalledFromGeneratedFile', '_SetListener', '__class__', '__deepcopy__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '_extensions_by_name', '_extensions_by_number', 'artifact_id', 'execution_id', 'milliseconds_since_epoch', 'path', 'system_metadata', 'type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get events related to output above\n",
        "execution_events = store.get_events_by_execution_ids([schema_events[0].execution_id])\n",
        "print(execution_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK5rVZcbm6Os",
        "outputId": "9b2feb52-0696-4633-b81e-2cdb53f4e336"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[artifact_id: 1\n",
            "execution_id: 1\n",
            "type: DECLARED_INPUT\n",
            "milliseconds_since_epoch: 1696623508360\n",
            ", artifact_id: 2\n",
            "execution_id: 1\n",
            "type: DECLARED_OUTPUT\n",
            "milliseconds_since_epoch: 1696623512993\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look up the artifact that is a declared input\n",
        "artifact_input = execution_events[0]\n",
        "\n",
        "store.get_artifacts_by_id([artifact_input.artifact_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfj-pnlqniQL",
        "outputId": "484a80e4-ecf9-4934-88d1-d40b04549697"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[id: 1\n",
              " type_id: 10\n",
              " uri: \"./data/train/data.csv\"\n",
              " properties {\n",
              "   key: \"name\"\n",
              "   value {\n",
              "     string_value: \"Chicago Taxi Dataset\"\n",
              "   }\n",
              " }\n",
              " properties {\n",
              "   key: \"split\"\n",
              "   value {\n",
              "     string_value: \"Train\"\n",
              "   }\n",
              " }\n",
              " properties {\n",
              "   key: \"version\"\n",
              "   value {\n",
              "     int_value: 1\n",
              "   }\n",
              " }\n",
              " type: \"DataSet\"\n",
              " create_time_since_epoch: 1696623508299\n",
              " last_update_time_since_epoch: 1696623508299]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sL237WgFnmF8"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}